# ProvablySafe.AI

**ProvablySafe.AI** is a collaborative landing page for the field and community
at the intersection of **AI safety** and **formal methods**.

Currently, the go-to introduction to the research field is
[Safeguarded AI: constructing safety by design](https://www.aria.org.uk/wp-content/uploads/2024/01/ARIA-Safeguarded-AI-Programme-Thesis-V1.pdf)
by David "davidad" Dalrymple.

## Community

Community resources and channels directly maintained by the ProvablySafe.AI
team:

- Zulip Chat/Forum: https://provablysafeai.zulipchat.com
- Newsletter: https://substack.provablysafe.ai
- Discord: https://discord.provablysafe.ai
- Github Organization: https://github.com/provablysafeai
- X/Twitter: https://twitter.com/ProvablySafeAI
- Bluesky: https://bsky.app/profile/provablysafe.ai

Other community resources:

- [**safe-by-design** mailing list](https://groups.google.com/g/safe-by-design)
  - Started by [Atlas Computing Initiative](https://atlascomputing.org/) in
    collaboration with [FAR AI](https://far.ai/)
  - Publicly readable
  - "We have new users request to join rather than enabling joining directly to
    prevent spammers, but we're very welcoming!"
- [**Formal Proof** tag on LessWrong](https://www.lesswrong.com/tag/formal-proof?sortedBy=new)
- The [Lean Prover](https://lean-lang.org/)
  [Zulip Chat](https://leanprover.zulipchat.com/) has a
  ["Machine Learning for Theorem Proving" stream](https://leanprover.zulipchat.com/#narrow/stream/219941-Machine-Learning-for-Theorem-Proving)
  - Conversations are mostly in the direction of applying machine learning to
    automate theorem proving, rather than applying formal methods to machine
    learning/AI safety, but progress in "AI4Math" is an important part of some
    "provably safe AI" proposals.

## Publications

### Programme thesis

- [Safeguarded AI: constructing safety by design](https://www.aria.org.uk/wp-content/uploads/2024/01/ARIA-Safeguarded-AI-Programme-Thesis-V1.pdf)
  (David “davidad” Dalrymple, 2024-01)

### Papers

- [Provably safe systems: the only path to controllable AGI](https://arxiv.org/abs/2309.01933)
  (2023-09-05)
- [Opening the AI black box: program synthesis via mechanistic interpretability](https://arxiv.org/abs/2402.05110)
  (2024-02-07)

### Forum posts

- [Davidad's Provably Safe AI Architecture - ARIA's Programme Thesis](https://www.lesswrong.com/posts/qweXJ6v9heSn4wvdk/davidad-s-provably-safe-ai-architecture-aria-s-programme)
  (LessWrong, 2024-02-01)
- [Provably Safe AI](https://www.lesswrong.com/posts/KX3Qwr7QM7CvhJLG6/provably-safe-ai)
  (LessWrong, 2023-10-05)
- [LOVE in a simbox is all you need](https://www.lesswrong.com/posts/WKGZBCYAbZ6WGsKHc/love-in-a-simbox-is-all-you-need)
  (LessWrong, 2022-09-28)
- [Open Agency Architecture _category_ on LessWrong](https://www.lesswrong.com/tag/open-agency-architecture)

### Resources

- [Collection of resources on AI for Math](https://docs.google.com/document/d/1kD7H4E28656ua8jOGZ934nbH2HcBLyxcRgFDduH5iQ0/edit),
  maintained by Talia Ringer

## Events

### Upcoming events

[Atlas Computing Initiative](https://atlascomputing.org/) is organizing
multiple events for the field in 2024:

- [Mathematical Boundaries](https://atlascomputing.org/events) – April 10-14
  Berkeley, CA
- [Conference on Provable Safety Properties](https://atlascomputing.org/) –
  June 6-8 in San Francisco Bay Area, CA

For AI safety-related events more generally, consider the
[AI Safety Events Tracker](https://aisafetyeventstracker.substack.com/).

### Past events

- [Systems Theory and Systems Practice Discussions Workshop](https://forest.localcharts.org/oxford-topos-meeting-2024.xml)
  – March 6-8, 2024 in Oxford, UK

## Media

### Videos

- [Safe AGI w/ Mechanistic Interpretability @ Harvard SEAS by Max Tegmark](https://www.youtube.com/watch?v=st9J6GefWeY)
  (2023-12-09)
- [How to Keep AI Under Control – TED talk by Max Tegmark](https://www.youtube.com/watch?v=xUNx_PxNHrY)
  (2023-11-02)
- [Controllable AI with Open Agency Systems @ Intelligent Cooperation Workshop by Evan Miyazono](https://www.youtube.com/watch?v=vEnysJ69Nto)
  (2023-08-22)

### Podcasts

- [Steve Omohundro on Provably Safe AGI @ Future of Life Institute](https://www.youtube.com/watch?v=YhMwkk6uOK8)
  (2023-10-05)

## About provablysafe.ai

**ProvablySafe.AI** is collaborative website for the field and community of
Safeguarded AI / Provably Safe AI.

### Objectives

- _Information hub_: aggregating public information on the field and community
  (papers, orgs, collaboration opportunities, events, …)
- _Field introduction_: Providing onboarding pathway(s) for newcomers
- Foster collaboration and progress

### Collaboration methodology and governance

Your contributions are very welcome! For updates, enhancements, bug fixes,
feedback:

- a)
  [Create a new issue](https://github.com/provablysafeai/website/issues/new);
  _or_
- b) Submit a pull request. The website is automatically published from the
  main branch to the domain.

The core maintainers periodically update the website, and process suggestions
(issues and PRs) on Github.

The
[_meta_ channel](https://provablysafeai.zulipchat.com/#narrow/stream/428550-org.2Fprovablysafeai.2Fmeta)
on Zulip enables for governance-level discussion for the project.

Caveat: Over the long-term, as per the research direction, a significant part
of the R&D will plausibly involve collaboration within private secure
environments which would not be public and therefore not be on a public website
or forum.

### Maintainers

- Helder S Ribeiro ([agentofuser.com](https://agentofuser.com/))
- Orpheus Lummis ([orpheuslummis.info](https://orpheuslummis.info/))

Reach us out on the [community Zulip](https://provablysafeai.zulipchat.com/).
